{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.engine import Layer\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, merge, concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import RepeatVector, Permute\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\yuki\\miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\n",
      "225214464/225209952 [==============================] - 47s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Get images\n",
    "X = [] \n",
    "for filename in os.listdir('images/Train/'):\n",
    "    X.append(img_to_array(load_img('images/Train/'+filename)))\n",
    "X = np.array(X, dtype=float)\n",
    "Xtrain = 1.0/255*X\n",
    "\n",
    "\n",
    "#Load weights\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)\n",
    "inception.graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_input = Input(shape=(1000,))\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\yuki\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 51s 51s/step - loss: 0.0475\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 22s 22s/step - loss: 0.9703\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9761\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.0216\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 20s 20s/step - loss: 1.0586\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 20s 20s/step - loss: 1.0002\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 20s 20s/step - loss: 0.9905\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9724\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.0071\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9906\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.0030\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9475\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9750\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9569\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.0300\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9546\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9820\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9393\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9846\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.0214\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.0047\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9756\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.8574\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.0308\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.0186\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9376\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9991\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9476\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9879\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 1.0149\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9951\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9431\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.0090\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9906\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9934\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9404\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9223\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.0165\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9732\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9340\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 14s 14s/step - loss: 1.0136\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9663\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9522\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9849\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 1.0055\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9871\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9719\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 18s 18s/step - loss: 0.9652\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 19s 19s/step - loss: 0.9461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x151b0af2208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = []\n",
    "    for i in grayscaled_rgb:\n",
    "        i = resize(i, (299, 299, 3), mode='constant')\n",
    "        grayscaled_rgb_resized.append(i)\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    with inception.graph.as_default():\n",
    "        embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "\n",
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        rotation_range=20,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "#Generate training data\n",
    "batch_size = 10\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        embed = create_inception_embedding(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:,:,:,0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "        Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "        yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n",
    "\n",
    "\n",
    "#Train model      \n",
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "model.fit_generator(image_a_b_gen(batch_size), epochs=50, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50/50 [==============================] - 922s 18s/step - loss: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x151b0b42390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(image_a_b_gen(batch_size), epochs=1, steps_per_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 56695 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\util\\dtype.py:141: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 51077 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 64004 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 13712 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 60319 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 40783 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 46570 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\io\\_io.py:140: UserWarning: result/img_6.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 50207 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 61192 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\io\\_io.py:140: UserWarning: result/img_8.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 49129 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 26031 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 41172 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\io\\_io.py:140: UserWarning: result/img_11.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 46292 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 52359 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\io\\_io.py:140: UserWarning: result/img_13.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 46739 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 34477 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 6699 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\io\\_io.py:140: UserWarning: result/img_16.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 31443 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 27345 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 58923 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 51248 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\io\\_io.py:140: UserWarning: result/img_20.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 49759 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 55573 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n",
      "c:\\users\\yuki\\miniconda3\\lib\\site-packages\\skimage\\color\\colorconv.py:993: UserWarning: Color data out of range: Z < 0 in 49678 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    }
   ],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir('images/Test/'):\n",
    "    color_me.append(img_to_array(load_img('images/Test/'+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]\n",
    "    cur[:,:,1:] = output[i]\n",
    "    imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
